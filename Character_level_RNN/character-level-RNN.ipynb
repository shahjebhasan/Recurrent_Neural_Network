{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character level LSTM in pytorch\n",
    "    In this notebook, I'll construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on JK ROWLING'S Harry Potter. This model will be able to generate new text based on the text from the book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/HarryPotter.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokeniztion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "{0: '=', 1: 'y', 2: 'W', 3: 'X', 4: '(', 5: '•', 6: 'V', 7: '$', 8: 'N', 9: '%', 10: '~', 11: 'I', 12: 'p', 13: 'K', 14: 't', 15: '9', 16: '6', 17: '_', 18: 'H', 19: '*', 20: 'w', 21: 'C', 22: 'O', 23: '!', 24: ')', 25: 'l', 26: '?', 27: '4', 28: 'B', 29: '8', 30: '\"', 31: 'b', 32: ']', 33: 'Y', 34: 'd', 35: 'E', 36: 'u', 37: 'T', 38: 'g', 39: 'r', 40: 'F', 41: '7', 42: '5', 43: ' ', 44: '/', 45: ':', 46: ',', 47: 'Q', 48: 'v', 49: 'a', 50: '-', 51: '1', 52: 'D', 53: 'm', 54: 'x', 55: 'A', 56: 'S', 57: 'c', 58: '0', 59: 'L', 60: 'U', 61: 'j', 62: 's', 63: 'h', 64: 'n', 65: 'Z', 66: '\\t', 67: 'P', 68: 'k', 69: '3', 70: 'J', 71: '`', 72: 'G', 73: 'z', 74: \"'\", 75: 'o', 76: '&', 77: 'i', 78: '\\n', 79: 'M', 80: '}', 81: 'R', 82: '.', 83: '2', 84: '\\\\', 85: ';', 86: 'f', 87: '^', 88: 'ü', 89: 'q', 90: 'e'}\n"
     ]
    }
   ],
   "source": [
    "chars=tuple(set(text))\n",
    "print(len(chars))\n",
    "integer2character = dict(enumerate(chars))\n",
    "print(integer2character)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'=': 0, 'y': 1, 'W': 2, 'X': 3, '(': 4, '•': 5, 'V': 6, '$': 7, 'N': 8, '%': 9, '~': 10, 'I': 11, 'p': 12, 'K': 13, 't': 14, '9': 15, '6': 16, '_': 17, 'H': 18, '*': 19, 'w': 20, 'C': 21, 'O': 22, '!': 23, ')': 24, 'l': 25, '?': 26, '4': 27, 'B': 28, '8': 29, '\"': 30, 'b': 31, ']': 32, 'Y': 33, 'd': 34, 'E': 35, 'u': 36, 'T': 37, 'g': 38, 'r': 39, 'F': 40, '7': 41, '5': 42, ' ': 43, '/': 44, ':': 45, ',': 46, 'Q': 47, 'v': 48, 'a': 49, '-': 50, '1': 51, 'D': 52, 'm': 53, 'x': 54, 'A': 55, 'S': 56, 'c': 57, '0': 58, 'L': 59, 'U': 60, 'j': 61, 's': 62, 'h': 63, 'n': 64, 'Z': 65, '\\t': 66, 'P': 67, 'k': 68, '3': 69, 'J': 70, '`': 71, 'G': 72, 'z': 73, \"'\": 74, 'o': 75, '&': 76, 'i': 77, '\\n': 78, 'M': 79, '}': 80, 'R': 81, '.': 82, '2': 83, '\\\\': 84, ';': 85, 'f': 86, '^': 87, 'ü': 88, 'q': 89, 'e': 90}\n"
     ]
    }
   ],
   "source": [
    "character2integer={ch:i for i,ch in integer2character.items()}\n",
    "print(character2integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90, 39, 43, 49, 64, 34, 43, 14, 63, 90, 43, 56, 75, 39, 57, 90, 39,\n",
       "       90, 39, 74, 62, 43, 56, 14, 75, 64, 90, 78, 78, 78, 21, 18, 55, 67,\n",
       "       37, 35, 81, 43, 22,  8, 35, 78, 78, 37, 18, 35, 43, 28, 22, 33, 43,\n",
       "        2, 18, 22, 43, 59, 11,  6, 35, 52, 78, 78, 79, 39, 82, 43, 49, 64,\n",
       "       34, 43, 79, 39, 62, 82, 43, 52, 36, 39, 62, 25, 90,  1, 46, 43, 75,\n",
       "       86, 43, 64, 36, 53, 31, 90, 39, 43, 86, 75, 36, 39, 46, 43, 67, 39,\n",
       "       77, 48, 90, 14, 43, 52, 39, 77, 48, 90, 46, 43, 20, 90, 39, 90, 43,\n",
       "       12, 39, 75, 36, 34, 43, 14, 75, 43, 62, 49,  1, 78, 14, 63, 49, 14,\n",
       "       43, 14, 63, 90])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_char=np.array([character2integer[ch] for ch in text])\n",
    "encoded_char[10:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, total_length):\n",
    "    one_hot=np.zeros((arr.size,total_length) ,dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    \n",
    "    one_hot=one_hot.reshape((*arr.shape,total_length))\n",
    "    \n",
    "    return one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make mini tarining batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr,batch_size,seq_length):\n",
    "    batches=batch_size*seq_length\n",
    "    num_batches=len(arr)//batches\n",
    "    arr=arr[:num_batches*batches]\n",
    "    arr=arr.reshape((batch_size,-1))\n",
    "    \n",
    "    for n in range(0,arr.shape[1],seq_length):\n",
    "        x=arr[:, n:n+seq_length]\n",
    "        y=np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1],y[:,-1]=x[:,1:],arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1],y[:,-1]=x[:,1:],arr[:,0]\n",
    "        yield x,y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 49 39 39  1 43 67 75 14 14]\n",
      " [50 43 63 75 20 43 14 90 39 43]\n",
      " [75 53 90 43 49 25 75 64 38 78]\n",
      " [49  1 46 43 49 64 34 43 57 25]\n",
      " [14 75 64 90 78 86 25 75 75 39]\n",
      " [78 30 79 75 39 64 77 64 38 23]\n",
      " [64 34 43 63 90 43 34 77 34 43]\n",
      " [39 43 86 39 75 53 43 14 63 90]]\n",
      "[[49 39 39  1 43 67 75 14 14 90]\n",
      " [43 63 75 20 43 14 90 39 43 39]\n",
      " [53 90 43 49 25 75 64 38 78 39]\n",
      " [ 1 46 43 49 64 34 43 57 25 75]\n",
      " [75 64 90 78 86 25 75 75 39 46]\n",
      " [30 79 75 39 64 77 64 38 23 30]\n",
      " [34 43 63 90 43 34 77 34 43 62]\n",
      " [43 86 39 75 53 43 14 63 90 43]]\n"
     ]
    }
   ],
   "source": [
    "batches=get_batches(encoded_char,8,50)\n",
    "x,y=next(batches)\n",
    "print(x[:10,:10])\n",
    "print(y[:10,:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu is available\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "if (train_on_gpu):\n",
    "    print(\"Gpu is available\")\n",
    "else:\n",
    "    print(\"Gpu is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure\n",
    "In __init__ the suggested structure is as follows:\n",
    "\n",
    ". Create and store the necessary dictionaries (this has been done for you)\n",
    ". Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size n_hidden, a number of layers n_layers, a dropout probability drop_prob, and a batch_first boolean (True, since we are batching)\n",
    ". Define a dropout layer with drop_prob\n",
    ". Define a fully-connected layer with params: input size n_hidden and output size (the number of characters)\n",
    ". Finally, initialize the weights (again, this has been given)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self,tokens,n_hidden=256,n_layers=2,drop_prob=0.4,lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob=drop_prob\n",
    "        self.n_layers=n_layers\n",
    "        self.n_hidden=n_hidden\n",
    "        self.lr=lr\n",
    "        \n",
    "        self.chars=tokens\n",
    "        self.int2char=dict(enumerate(self.chars))\n",
    "        self.char2int={ch:i for i,ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,dropout=drop_prob,batch_first=True)\n",
    "        self.dropout=nn.Dropout(drop_prob)\n",
    "        self.fc=nn.Linear(n_hidden,len(self.chars))\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        r_output,hidden=self.lstm(x,hidden)\n",
    "        output=self.dropout(r_output)\n",
    "        output=output.contiguous().view(-1,self.n_hidden)\n",
    "        output=self.fc(output)\n",
    "        return output,hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All right. Time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,data,epochs=10,batch_size=10,seq_length=50,lr=0.001,clip=5,val_frac=0.1,print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx=int(len(data)*(1-val_frac))\n",
    "    data,val_data=data[:val_idx],data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    counter=0\n",
    "    n_chars=len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        ## initialize hidden state\n",
    "        h=net.init_hidden(batch_size)\n",
    "        \n",
    "        for x,y in get_batches(data,batch_size,seq_length):\n",
    "            counter+=1\n",
    "            x=one_hot_encoder(x,n_chars)\n",
    "            inputs,targets=torch.from_numpy(x),torch.from_numpy(y)\n",
    "            \n",
    "            if (train_on_gpu):\n",
    "                inputs,targets = inputs.cuda(),targets.cuda()\n",
    "            ##creating new variables for hidden state\n",
    "            h=tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            output,h=net(inputs,h)\n",
    "            loss=criterion(output,targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(),clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encoder(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(91, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (fc): Linear(in_features=512, out_features=91, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "network=CharRNN(chars,n_hidden,n_layers)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 1.8889... Val Loss: 1.7365\n",
      "Epoch: 1/20... Step: 20... Loss: 1.7926... Val Loss: 1.7014\n",
      "Epoch: 1/20... Step: 30... Loss: 1.7967... Val Loss: 1.6854\n",
      "Epoch: 1/20... Step: 40... Loss: 1.8029... Val Loss: 1.6760\n",
      "Epoch: 1/20... Step: 50... Loss: 1.8280... Val Loss: 1.6667\n",
      "Epoch: 1/20... Step: 60... Loss: 1.7917... Val Loss: 1.6567\n",
      "Epoch: 1/20... Step: 70... Loss: 1.7900... Val Loss: 1.6501\n",
      "Epoch: 1/20... Step: 80... Loss: 1.7708... Val Loss: 1.6414\n",
      "Epoch: 1/20... Step: 90... Loss: 1.7500... Val Loss: 1.6344\n",
      "Epoch: 1/20... Step: 100... Loss: 1.7580... Val Loss: 1.6340\n",
      "Epoch: 1/20... Step: 110... Loss: 1.7510... Val Loss: 1.6238\n",
      "Epoch: 1/20... Step: 120... Loss: 1.7142... Val Loss: 1.6161\n",
      "Epoch: 1/20... Step: 130... Loss: 1.7158... Val Loss: 1.6130\n",
      "Epoch: 1/20... Step: 140... Loss: 1.7394... Val Loss: 1.6095\n",
      "Epoch: 1/20... Step: 150... Loss: 1.7291... Val Loss: 1.5999\n",
      "Epoch: 1/20... Step: 160... Loss: 1.7361... Val Loss: 1.5923\n",
      "Epoch: 1/20... Step: 170... Loss: 1.7617... Val Loss: 1.5864\n",
      "Epoch: 1/20... Step: 180... Loss: 1.6970... Val Loss: 1.5806\n",
      "Epoch: 1/20... Step: 190... Loss: 1.6813... Val Loss: 1.5714\n",
      "Epoch: 1/20... Step: 200... Loss: 1.7246... Val Loss: 1.5698\n",
      "Epoch: 1/20... Step: 210... Loss: 1.6860... Val Loss: 1.5601\n",
      "Epoch: 1/20... Step: 220... Loss: 1.6348... Val Loss: 1.5508\n",
      "Epoch: 1/20... Step: 230... Loss: 1.6533... Val Loss: 1.5443\n",
      "Epoch: 1/20... Step: 240... Loss: 1.6644... Val Loss: 1.5451\n",
      "Epoch: 1/20... Step: 250... Loss: 1.6684... Val Loss: 1.5327\n",
      "Epoch: 1/20... Step: 260... Loss: 1.7183... Val Loss: 1.5253\n",
      "Epoch: 1/20... Step: 270... Loss: 1.6227... Val Loss: 1.5191\n",
      "Epoch: 1/20... Step: 280... Loss: 1.6195... Val Loss: 1.5080\n",
      "Epoch: 1/20... Step: 290... Loss: 1.6788... Val Loss: 1.5115\n",
      "Epoch: 2/20... Step: 300... Loss: 1.6374... Val Loss: 1.5058\n",
      "Epoch: 2/20... Step: 310... Loss: 1.6493... Val Loss: 1.4999\n",
      "Epoch: 2/20... Step: 320... Loss: 1.6029... Val Loss: 1.4960\n",
      "Epoch: 2/20... Step: 330... Loss: 1.5957... Val Loss: 1.4925\n",
      "Epoch: 2/20... Step: 340... Loss: 1.6514... Val Loss: 1.4844\n",
      "Epoch: 2/20... Step: 350... Loss: 1.5941... Val Loss: 1.4778\n",
      "Epoch: 2/20... Step: 360... Loss: 1.6175... Val Loss: 1.4729\n",
      "Epoch: 2/20... Step: 370... Loss: 1.5747... Val Loss: 1.4687\n",
      "Epoch: 2/20... Step: 380... Loss: 1.5641... Val Loss: 1.4648\n",
      "Epoch: 2/20... Step: 390... Loss: 1.5753... Val Loss: 1.4626\n",
      "Epoch: 2/20... Step: 400... Loss: 1.5712... Val Loss: 1.4582\n",
      "Epoch: 2/20... Step: 410... Loss: 1.5582... Val Loss: 1.4532\n",
      "Epoch: 2/20... Step: 420... Loss: 1.5616... Val Loss: 1.4521\n",
      "Epoch: 2/20... Step: 430... Loss: 1.5744... Val Loss: 1.4476\n",
      "Epoch: 2/20... Step: 440... Loss: 1.5601... Val Loss: 1.4395\n",
      "Epoch: 2/20... Step: 450... Loss: 1.5699... Val Loss: 1.4362\n",
      "Epoch: 2/20... Step: 460... Loss: 1.5870... Val Loss: 1.4324\n",
      "Epoch: 2/20... Step: 470... Loss: 1.5532... Val Loss: 1.4302\n",
      "Epoch: 2/20... Step: 480... Loss: 1.5318... Val Loss: 1.4285\n",
      "Epoch: 2/20... Step: 490... Loss: 1.5840... Val Loss: 1.4213\n",
      "Epoch: 2/20... Step: 500... Loss: 1.5374... Val Loss: 1.4163\n",
      "Epoch: 2/20... Step: 510... Loss: 1.5240... Val Loss: 1.4132\n",
      "Epoch: 2/20... Step: 520... Loss: 1.5115... Val Loss: 1.4074\n",
      "Epoch: 2/20... Step: 530... Loss: 1.5259... Val Loss: 1.4041\n",
      "Epoch: 2/20... Step: 540... Loss: 1.4925... Val Loss: 1.3953\n",
      "Epoch: 2/20... Step: 550... Loss: 1.5195... Val Loss: 1.3913\n",
      "Epoch: 2/20... Step: 560... Loss: 1.5350... Val Loss: 1.3852\n",
      "Epoch: 2/20... Step: 570... Loss: 1.4742... Val Loss: 1.3835\n",
      "Epoch: 2/20... Step: 580... Loss: 1.5438... Val Loss: 1.3808\n",
      "Epoch: 3/20... Step: 590... Loss: 1.5344... Val Loss: 1.3789\n",
      "Epoch: 3/20... Step: 600... Loss: 1.4511... Val Loss: 1.3781\n",
      "Epoch: 3/20... Step: 610... Loss: 1.4895... Val Loss: 1.3721\n",
      "Epoch: 3/20... Step: 620... Loss: 1.4575... Val Loss: 1.3720\n",
      "Epoch: 3/20... Step: 630... Loss: 1.5343... Val Loss: 1.3694\n",
      "Epoch: 3/20... Step: 640... Loss: 1.4910... Val Loss: 1.3634\n",
      "Epoch: 3/20... Step: 650... Loss: 1.5115... Val Loss: 1.3615\n",
      "Epoch: 3/20... Step: 660... Loss: 1.4159... Val Loss: 1.3596\n",
      "Epoch: 3/20... Step: 670... Loss: 1.4307... Val Loss: 1.3573\n",
      "Epoch: 3/20... Step: 680... Loss: 1.4395... Val Loss: 1.3544\n",
      "Epoch: 3/20... Step: 690... Loss: 1.4836... Val Loss: 1.3550\n",
      "Epoch: 3/20... Step: 700... Loss: 1.4048... Val Loss: 1.3537\n",
      "Epoch: 3/20... Step: 710... Loss: 1.4190... Val Loss: 1.3474\n",
      "Epoch: 3/20... Step: 720... Loss: 1.4710... Val Loss: 1.3467\n",
      "Epoch: 3/20... Step: 730... Loss: 1.4428... Val Loss: 1.3387\n",
      "Epoch: 3/20... Step: 740... Loss: 1.4937... Val Loss: 1.3378\n",
      "Epoch: 3/20... Step: 750... Loss: 1.5073... Val Loss: 1.3370\n",
      "Epoch: 3/20... Step: 760... Loss: 1.4531... Val Loss: 1.3355\n",
      "Epoch: 3/20... Step: 770... Loss: 1.4949... Val Loss: 1.3304\n",
      "Epoch: 3/20... Step: 780... Loss: 1.4624... Val Loss: 1.3282\n",
      "Epoch: 3/20... Step: 790... Loss: 1.4410... Val Loss: 1.3253\n",
      "Epoch: 3/20... Step: 800... Loss: 1.4170... Val Loss: 1.3223\n",
      "Epoch: 3/20... Step: 810... Loss: 1.4184... Val Loss: 1.3192\n",
      "Epoch: 3/20... Step: 820... Loss: 1.3975... Val Loss: 1.3182\n",
      "Epoch: 3/20... Step: 830... Loss: 1.4835... Val Loss: 1.3152\n",
      "Epoch: 3/20... Step: 840... Loss: 1.4573... Val Loss: 1.3116\n",
      "Epoch: 3/20... Step: 850... Loss: 1.4178... Val Loss: 1.3049\n",
      "Epoch: 3/20... Step: 860... Loss: 1.4079... Val Loss: 1.3022\n",
      "Epoch: 3/20... Step: 870... Loss: 1.4128... Val Loss: 1.3034\n",
      "Epoch: 4/20... Step: 880... Loss: 1.4460... Val Loss: 1.3005\n",
      "Epoch: 4/20... Step: 890... Loss: 1.4413... Val Loss: 1.3009\n",
      "Epoch: 4/20... Step: 900... Loss: 1.3928... Val Loss: 1.3010\n",
      "Epoch: 4/20... Step: 910... Loss: 1.4018... Val Loss: 1.2945\n",
      "Epoch: 4/20... Step: 920... Loss: 1.4368... Val Loss: 1.2985\n",
      "Epoch: 4/20... Step: 930... Loss: 1.3964... Val Loss: 1.2918\n",
      "Epoch: 4/20... Step: 940... Loss: 1.4047... Val Loss: 1.2913\n",
      "Epoch: 4/20... Step: 950... Loss: 1.4080... Val Loss: 1.2909\n",
      "Epoch: 4/20... Step: 960... Loss: 1.4115... Val Loss: 1.2883\n",
      "Epoch: 4/20... Step: 970... Loss: 1.3996... Val Loss: 1.2884\n",
      "Epoch: 4/20... Step: 980... Loss: 1.4099... Val Loss: 1.2842\n",
      "Epoch: 4/20... Step: 990... Loss: 1.3813... Val Loss: 1.2866\n",
      "Epoch: 4/20... Step: 1000... Loss: 1.3345... Val Loss: 1.2826\n",
      "Epoch: 4/20... Step: 1010... Loss: 1.3978... Val Loss: 1.2843\n",
      "Epoch: 4/20... Step: 1020... Loss: 1.3444... Val Loss: 1.2820\n",
      "Epoch: 4/20... Step: 1030... Loss: 1.4064... Val Loss: 1.2775\n",
      "Epoch: 4/20... Step: 1040... Loss: 1.3800... Val Loss: 1.2761\n",
      "Epoch: 4/20... Step: 1050... Loss: 1.4172... Val Loss: 1.2768\n",
      "Epoch: 4/20... Step: 1060... Loss: 1.3676... Val Loss: 1.2729\n",
      "Epoch: 4/20... Step: 1070... Loss: 1.3931... Val Loss: 1.2718\n",
      "Epoch: 4/20... Step: 1080... Loss: 1.3599... Val Loss: 1.2685\n",
      "Epoch: 4/20... Step: 1090... Loss: 1.3915... Val Loss: 1.2690\n",
      "Epoch: 4/20... Step: 1100... Loss: 1.3601... Val Loss: 1.2664\n",
      "Epoch: 4/20... Step: 1110... Loss: 1.3792... Val Loss: 1.2666\n",
      "Epoch: 4/20... Step: 1120... Loss: 1.3622... Val Loss: 1.2601\n",
      "Epoch: 4/20... Step: 1130... Loss: 1.4069... Val Loss: 1.2612\n",
      "Epoch: 4/20... Step: 1140... Loss: 1.3570... Val Loss: 1.2552\n",
      "Epoch: 4/20... Step: 1150... Loss: 1.3971... Val Loss: 1.2515\n",
      "Epoch: 4/20... Step: 1160... Loss: 1.4017... Val Loss: 1.2535\n",
      "Epoch: 5/20... Step: 1170... Loss: 1.3877... Val Loss: 1.2514\n",
      "Epoch: 5/20... Step: 1180... Loss: 1.3368... Val Loss: 1.2544\n",
      "Epoch: 5/20... Step: 1190... Loss: 1.3381... Val Loss: 1.2550\n",
      "Epoch: 5/20... Step: 1200... Loss: 1.3468... Val Loss: 1.2496\n",
      "Epoch: 5/20... Step: 1210... Loss: 1.3690... Val Loss: 1.2515\n",
      "Epoch: 5/20... Step: 1220... Loss: 1.3233... Val Loss: 1.2507\n",
      "Epoch: 5/20... Step: 1230... Loss: 1.3623... Val Loss: 1.2467\n",
      "Epoch: 5/20... Step: 1240... Loss: 1.3165... Val Loss: 1.2454\n",
      "Epoch: 5/20... Step: 1250... Loss: 1.3619... Val Loss: 1.2428\n",
      "Epoch: 5/20... Step: 1260... Loss: 1.3501... Val Loss: 1.2485\n",
      "Epoch: 5/20... Step: 1270... Loss: 1.3642... Val Loss: 1.2439\n",
      "Epoch: 5/20... Step: 1280... Loss: 1.3262... Val Loss: 1.2460\n",
      "Epoch: 5/20... Step: 1290... Loss: 1.3311... Val Loss: 1.2406\n",
      "Epoch: 5/20... Step: 1300... Loss: 1.4067... Val Loss: 1.2419\n",
      "Epoch: 5/20... Step: 1310... Loss: 1.3351... Val Loss: 1.2390\n",
      "Epoch: 5/20... Step: 1320... Loss: 1.3783... Val Loss: 1.2389\n",
      "Epoch: 5/20... Step: 1330... Loss: 1.3507... Val Loss: 1.2364\n",
      "Epoch: 5/20... Step: 1340... Loss: 1.3455... Val Loss: 1.2367\n",
      "Epoch: 5/20... Step: 1350... Loss: 1.3432... Val Loss: 1.2368\n",
      "Epoch: 5/20... Step: 1360... Loss: 1.3413... Val Loss: 1.2338\n",
      "Epoch: 5/20... Step: 1370... Loss: 1.3295... Val Loss: 1.2311\n",
      "Epoch: 5/20... Step: 1380... Loss: 1.3157... Val Loss: 1.2297\n",
      "Epoch: 5/20... Step: 1390... Loss: 1.3236... Val Loss: 1.2278\n",
      "Epoch: 5/20... Step: 1400... Loss: 1.3699... Val Loss: 1.2267\n",
      "Epoch: 5/20... Step: 1410... Loss: 1.3264... Val Loss: 1.2244\n",
      "Epoch: 5/20... Step: 1420... Loss: 1.3339... Val Loss: 1.2246\n",
      "Epoch: 5/20... Step: 1430... Loss: 1.2986... Val Loss: 1.2179\n",
      "Epoch: 5/20... Step: 1440... Loss: 1.3331... Val Loss: 1.2173\n",
      "Epoch: 5/20... Step: 1450... Loss: 1.3191... Val Loss: 1.2181\n",
      "Epoch: 6/20... Step: 1460... Loss: 1.3039... Val Loss: 1.2195\n",
      "Epoch: 6/20... Step: 1470... Loss: 1.2991... Val Loss: 1.2216\n",
      "Epoch: 6/20... Step: 1480... Loss: 1.3206... Val Loss: 1.2199\n",
      "Epoch: 6/20... Step: 1490... Loss: 1.2750... Val Loss: 1.2148\n",
      "Epoch: 6/20... Step: 1500... Loss: 1.3266... Val Loss: 1.2175\n",
      "Epoch: 6/20... Step: 1510... Loss: 1.3140... Val Loss: 1.2180\n",
      "Epoch: 6/20... Step: 1520... Loss: 1.3612... Val Loss: 1.2143\n",
      "Epoch: 6/20... Step: 1530... Loss: 1.3022... Val Loss: 1.2131\n",
      "Epoch: 6/20... Step: 1540... Loss: 1.3251... Val Loss: 1.2133\n",
      "Epoch: 6/20... Step: 1550... Loss: 1.3019... Val Loss: 1.2164\n",
      "Epoch: 6/20... Step: 1560... Loss: 1.2912... Val Loss: 1.2130\n",
      "Epoch: 6/20... Step: 1570... Loss: 1.2771... Val Loss: 1.2124\n",
      "Epoch: 6/20... Step: 1580... Loss: 1.2967... Val Loss: 1.2114\n",
      "Epoch: 6/20... Step: 1590... Loss: 1.3420... Val Loss: 1.2142\n",
      "Epoch: 6/20... Step: 1600... Loss: 1.3083... Val Loss: 1.2084\n",
      "Epoch: 6/20... Step: 1610... Loss: 1.3179... Val Loss: 1.2104\n",
      "Epoch: 6/20... Step: 1620... Loss: 1.2711... Val Loss: 1.2070\n",
      "Epoch: 6/20... Step: 1630... Loss: 1.2998... Val Loss: 1.2117\n",
      "Epoch: 6/20... Step: 1640... Loss: 1.3006... Val Loss: 1.2101\n",
      "Epoch: 6/20... Step: 1650... Loss: 1.3161... Val Loss: 1.2064\n",
      "Epoch: 6/20... Step: 1660... Loss: 1.3054... Val Loss: 1.2032\n",
      "Epoch: 6/20... Step: 1670... Loss: 1.2870... Val Loss: 1.2031\n",
      "Epoch: 6/20... Step: 1680... Loss: 1.2743... Val Loss: 1.2010\n",
      "Epoch: 6/20... Step: 1690... Loss: 1.2879... Val Loss: 1.1991\n",
      "Epoch: 6/20... Step: 1700... Loss: 1.2852... Val Loss: 1.2011\n",
      "Epoch: 6/20... Step: 1710... Loss: 1.3060... Val Loss: 1.1995\n",
      "Epoch: 6/20... Step: 1720... Loss: 1.2806... Val Loss: 1.1938\n",
      "Epoch: 6/20... Step: 1730... Loss: 1.2717... Val Loss: 1.1947\n",
      "Epoch: 6/20... Step: 1740... Loss: 1.2976... Val Loss: 1.1906\n",
      "Epoch: 7/20... Step: 1750... Loss: 1.2645... Val Loss: 1.1937\n",
      "Epoch: 7/20... Step: 1760... Loss: 1.2880... Val Loss: 1.1973\n",
      "Epoch: 7/20... Step: 1770... Loss: 1.2706... Val Loss: 1.1914\n",
      "Epoch: 7/20... Step: 1780... Loss: 1.3453... Val Loss: 1.1935\n",
      "Epoch: 7/20... Step: 1790... Loss: 1.2870... Val Loss: 1.1942\n",
      "Epoch: 7/20... Step: 1800... Loss: 1.2933... Val Loss: 1.1944\n",
      "Epoch: 7/20... Step: 1810... Loss: 1.3078... Val Loss: 1.1920\n",
      "Epoch: 7/20... Step: 1820... Loss: 1.2562... Val Loss: 1.1921\n",
      "Epoch: 7/20... Step: 1830... Loss: 1.2588... Val Loss: 1.1894\n",
      "Epoch: 7/20... Step: 1840... Loss: 1.2434... Val Loss: 1.1889\n",
      "Epoch: 7/20... Step: 1850... Loss: 1.2897... Val Loss: 1.1899\n",
      "Epoch: 7/20... Step: 1860... Loss: 1.2485... Val Loss: 1.1923\n",
      "Epoch: 7/20... Step: 1870... Loss: 1.2969... Val Loss: 1.1886\n",
      "Epoch: 7/20... Step: 1880... Loss: 1.2494... Val Loss: 1.1907\n",
      "Epoch: 7/20... Step: 1890... Loss: 1.2863... Val Loss: 1.1869\n",
      "Epoch: 7/20... Step: 1900... Loss: 1.2564... Val Loss: 1.1886\n",
      "Epoch: 7/20... Step: 1910... Loss: 1.2580... Val Loss: 1.1862\n",
      "Epoch: 7/20... Step: 1920... Loss: 1.2840... Val Loss: 1.1868\n",
      "Epoch: 7/20... Step: 1930... Loss: 1.2811... Val Loss: 1.1855\n",
      "Epoch: 7/20... Step: 1940... Loss: 1.2675... Val Loss: 1.1900\n",
      "Epoch: 7/20... Step: 1950... Loss: 1.2512... Val Loss: 1.1843\n",
      "Epoch: 7/20... Step: 1960... Loss: 1.2782... Val Loss: 1.1819\n",
      "Epoch: 7/20... Step: 1970... Loss: 1.2489... Val Loss: 1.1823\n",
      "Epoch: 7/20... Step: 1980... Loss: 1.2653... Val Loss: 1.1782\n",
      "Epoch: 7/20... Step: 1990... Loss: 1.2500... Val Loss: 1.1791\n",
      "Epoch: 7/20... Step: 2000... Loss: 1.3018... Val Loss: 1.1786\n",
      "Epoch: 7/20... Step: 2010... Loss: 1.2826... Val Loss: 1.1743\n",
      "Epoch: 7/20... Step: 2020... Loss: 1.2693... Val Loss: 1.1736\n",
      "Epoch: 7/20... Step: 2030... Loss: 1.2342... Val Loss: 1.1725\n",
      "Epoch: 8/20... Step: 2040... Loss: 1.2679... Val Loss: 1.1748\n",
      "Epoch: 8/20... Step: 2050... Loss: 1.2622... Val Loss: 1.1756\n",
      "Epoch: 8/20... Step: 2060... Loss: 1.2679... Val Loss: 1.1740\n",
      "Epoch: 8/20... Step: 2070... Loss: 1.2686... Val Loss: 1.1758\n",
      "Epoch: 8/20... Step: 2080... Loss: 1.2633... Val Loss: 1.1759\n",
      "Epoch: 8/20... Step: 2090... Loss: 1.2354... Val Loss: 1.1781\n",
      "Epoch: 8/20... Step: 2100... Loss: 1.2568... Val Loss: 1.1734\n",
      "Epoch: 8/20... Step: 2110... Loss: 1.2246... Val Loss: 1.1733\n",
      "Epoch: 8/20... Step: 2120... Loss: 1.2406... Val Loss: 1.1728\n",
      "Epoch: 8/20... Step: 2130... Loss: 1.2303... Val Loss: 1.1741\n",
      "Epoch: 8/20... Step: 2140... Loss: 1.2427... Val Loss: 1.1732\n",
      "Epoch: 8/20... Step: 2150... Loss: 1.2272... Val Loss: 1.1728\n",
      "Epoch: 8/20... Step: 2160... Loss: 1.2254... Val Loss: 1.1706\n",
      "Epoch: 8/20... Step: 2170... Loss: 1.2229... Val Loss: 1.1733\n",
      "Epoch: 8/20... Step: 2180... Loss: 1.2692... Val Loss: 1.1726\n",
      "Epoch: 8/20... Step: 2190... Loss: 1.2917... Val Loss: 1.1713\n",
      "Epoch: 8/20... Step: 2200... Loss: 1.2227... Val Loss: 1.1707\n",
      "Epoch: 8/20... Step: 2210... Loss: 1.2299... Val Loss: 1.1699\n",
      "Epoch: 8/20... Step: 2220... Loss: 1.2497... Val Loss: 1.1690\n",
      "Epoch: 8/20... Step: 2230... Loss: 1.2281... Val Loss: 1.1708\n",
      "Epoch: 8/20... Step: 2240... Loss: 1.2160... Val Loss: 1.1657\n",
      "Epoch: 8/20... Step: 2250... Loss: 1.2125... Val Loss: 1.1666\n",
      "Epoch: 8/20... Step: 2260... Loss: 1.2521... Val Loss: 1.1662\n",
      "Epoch: 8/20... Step: 2270... Loss: 1.2116... Val Loss: 1.1651\n",
      "Epoch: 8/20... Step: 2280... Loss: 1.2437... Val Loss: 1.1622\n",
      "Epoch: 8/20... Step: 2290... Loss: 1.2601... Val Loss: 1.1615\n",
      "Epoch: 8/20... Step: 2300... Loss: 1.2531... Val Loss: 1.1570\n",
      "Epoch: 8/20... Step: 2310... Loss: 1.2323... Val Loss: 1.1570\n",
      "Epoch: 8/20... Step: 2320... Loss: 1.2310... Val Loss: 1.1589\n",
      "Epoch: 9/20... Step: 2330... Loss: 1.2571... Val Loss: 1.1601\n",
      "Epoch: 9/20... Step: 2340... Loss: 1.2534... Val Loss: 1.1600\n",
      "Epoch: 9/20... Step: 2350... Loss: 1.2382... Val Loss: 1.1607\n",
      "Epoch: 9/20... Step: 2360... Loss: 1.2100... Val Loss: 1.1619\n",
      "Epoch: 9/20... Step: 2370... Loss: 1.2270... Val Loss: 1.1607\n",
      "Epoch: 9/20... Step: 2380... Loss: 1.2816... Val Loss: 1.1612\n",
      "Epoch: 9/20... Step: 2390... Loss: 1.2475... Val Loss: 1.1583\n",
      "Epoch: 9/20... Step: 2400... Loss: 1.2695... Val Loss: 1.1617\n",
      "Epoch: 9/20... Step: 2410... Loss: 1.2238... Val Loss: 1.1609\n",
      "Epoch: 9/20... Step: 2420... Loss: 1.2135... Val Loss: 1.1586\n",
      "Epoch: 9/20... Step: 2430... Loss: 1.2136... Val Loss: 1.1597\n",
      "Epoch: 9/20... Step: 2440... Loss: 1.2105... Val Loss: 1.1615\n",
      "Epoch: 9/20... Step: 2450... Loss: 1.1802... Val Loss: 1.1584\n",
      "Epoch: 9/20... Step: 2460... Loss: 1.2044... Val Loss: 1.1596\n",
      "Epoch: 9/20... Step: 2470... Loss: 1.2137... Val Loss: 1.1601\n",
      "Epoch: 9/20... Step: 2480... Loss: 1.2441... Val Loss: 1.1577\n",
      "Epoch: 9/20... Step: 2490... Loss: 1.2525... Val Loss: 1.1540\n",
      "Epoch: 9/20... Step: 2500... Loss: 1.2461... Val Loss: 1.1575\n",
      "Epoch: 9/20... Step: 2510... Loss: 1.2320... Val Loss: 1.1573\n",
      "Epoch: 9/20... Step: 2520... Loss: 1.2216... Val Loss: 1.1558\n",
      "Epoch: 9/20... Step: 2530... Loss: 1.2202... Val Loss: 1.1524\n",
      "Epoch: 9/20... Step: 2540... Loss: 1.2779... Val Loss: 1.1527\n",
      "Epoch: 9/20... Step: 2550... Loss: 1.1685... Val Loss: 1.1509\n",
      "Epoch: 9/20... Step: 2560... Loss: 1.2082... Val Loss: 1.1499\n",
      "Epoch: 9/20... Step: 2570... Loss: 1.2206... Val Loss: 1.1518\n",
      "Epoch: 9/20... Step: 2580... Loss: 1.2610... Val Loss: 1.1489\n",
      "Epoch: 9/20... Step: 2590... Loss: 1.2214... Val Loss: 1.1475\n",
      "Epoch: 9/20... Step: 2600... Loss: 1.2340... Val Loss: 1.1451\n",
      "Epoch: 9/20... Step: 2610... Loss: 1.1920... Val Loss: 1.1472\n",
      "Epoch: 10/20... Step: 2620... Loss: 1.2701... Val Loss: 1.1483\n",
      "Epoch: 10/20... Step: 2630... Loss: 1.2092... Val Loss: 1.1502\n",
      "Epoch: 10/20... Step: 2640... Loss: 1.2022... Val Loss: 1.1492\n",
      "Epoch: 10/20... Step: 2650... Loss: 1.2367... Val Loss: 1.1492\n",
      "Epoch: 10/20... Step: 2660... Loss: 1.2069... Val Loss: 1.1461\n",
      "Epoch: 10/20... Step: 2670... Loss: 1.2351... Val Loss: 1.1486\n",
      "Epoch: 10/20... Step: 2680... Loss: 1.1917... Val Loss: 1.1462\n",
      "Epoch: 10/20... Step: 2690... Loss: 1.2254... Val Loss: 1.1465\n",
      "Epoch: 10/20... Step: 2700... Loss: 1.1778... Val Loss: 1.1475\n",
      "Epoch: 10/20... Step: 2710... Loss: 1.1916... Val Loss: 1.1459\n",
      "Epoch: 10/20... Step: 2720... Loss: 1.2356... Val Loss: 1.1462\n",
      "Epoch: 10/20... Step: 2730... Loss: 1.2210... Val Loss: 1.1503\n",
      "Epoch: 10/20... Step: 2740... Loss: 1.1512... Val Loss: 1.1513\n",
      "Epoch: 10/20... Step: 2750... Loss: 1.1711... Val Loss: 1.1470\n",
      "Epoch: 10/20... Step: 2760... Loss: 1.1899... Val Loss: 1.1505\n",
      "Epoch: 10/20... Step: 2770... Loss: 1.2225... Val Loss: 1.1464\n",
      "Epoch: 10/20... Step: 2780... Loss: 1.2406... Val Loss: 1.1437\n",
      "Epoch: 10/20... Step: 2790... Loss: 1.2621... Val Loss: 1.1470\n",
      "Epoch: 10/20... Step: 2800... Loss: 1.2313... Val Loss: 1.1443\n",
      "Epoch: 10/20... Step: 2810... Loss: 1.2216... Val Loss: 1.1450\n",
      "Epoch: 10/20... Step: 2820... Loss: 1.1849... Val Loss: 1.1428\n",
      "Epoch: 10/20... Step: 2830... Loss: 1.1661... Val Loss: 1.1407\n",
      "Epoch: 10/20... Step: 2840... Loss: 1.1916... Val Loss: 1.1412\n",
      "Epoch: 10/20... Step: 2850... Loss: 1.1673... Val Loss: 1.1389\n",
      "Epoch: 10/20... Step: 2860... Loss: 1.1954... Val Loss: 1.1414\n",
      "Epoch: 10/20... Step: 2870... Loss: 1.2093... Val Loss: 1.1405\n",
      "Epoch: 10/20... Step: 2880... Loss: 1.2068... Val Loss: 1.1377\n",
      "Epoch: 10/20... Step: 2890... Loss: 1.2121... Val Loss: 1.1387\n",
      "Epoch: 10/20... Step: 2900... Loss: 1.1851... Val Loss: 1.1369\n",
      "Epoch: 10/20... Step: 2910... Loss: 1.2224... Val Loss: 1.1397\n",
      "Epoch: 11/20... Step: 2920... Loss: 1.2180... Val Loss: 1.1387\n",
      "Epoch: 11/20... Step: 2930... Loss: 1.1534... Val Loss: 1.1392\n",
      "Epoch: 11/20... Step: 2940... Loss: 1.1967... Val Loss: 1.1422\n",
      "Epoch: 11/20... Step: 2950... Loss: 1.2131... Val Loss: 1.1361\n",
      "Epoch: 11/20... Step: 2960... Loss: 1.2615... Val Loss: 1.1382\n",
      "Epoch: 11/20... Step: 2970... Loss: 1.2016... Val Loss: 1.1371\n",
      "Epoch: 11/20... Step: 2980... Loss: 1.1806... Val Loss: 1.1383\n",
      "Epoch: 11/20... Step: 2990... Loss: 1.1810... Val Loss: 1.1368\n",
      "Epoch: 11/20... Step: 3000... Loss: 1.1914... Val Loss: 1.1352\n",
      "Epoch: 11/20... Step: 3010... Loss: 1.1836... Val Loss: 1.1376\n",
      "Epoch: 11/20... Step: 3020... Loss: 1.2019... Val Loss: 1.1397\n",
      "Epoch: 11/20... Step: 3030... Loss: 1.1584... Val Loss: 1.1370\n",
      "Epoch: 11/20... Step: 3040... Loss: 1.1848... Val Loss: 1.1366\n",
      "Epoch: 11/20... Step: 3050... Loss: 1.1986... Val Loss: 1.1406\n",
      "Epoch: 11/20... Step: 3060... Loss: 1.2130... Val Loss: 1.1347\n",
      "Epoch: 11/20... Step: 3070... Loss: 1.2173... Val Loss: 1.1332\n",
      "Epoch: 11/20... Step: 3080... Loss: 1.2476... Val Loss: 1.1342\n",
      "Epoch: 11/20... Step: 3090... Loss: 1.1805... Val Loss: 1.1343\n",
      "Epoch: 11/20... Step: 3100... Loss: 1.1571... Val Loss: 1.1325\n",
      "Epoch: 11/20... Step: 3110... Loss: 1.2164... Val Loss: 1.1325\n",
      "Epoch: 11/20... Step: 3120... Loss: 1.1810... Val Loss: 1.1326\n",
      "Epoch: 11/20... Step: 3130... Loss: 1.1852... Val Loss: 1.1326\n",
      "Epoch: 11/20... Step: 3140... Loss: 1.1881... Val Loss: 1.1326\n",
      "Epoch: 11/20... Step: 3150... Loss: 1.1885... Val Loss: 1.1294\n",
      "Epoch: 11/20... Step: 3160... Loss: 1.2111... Val Loss: 1.1297\n",
      "Epoch: 11/20... Step: 3170... Loss: 1.2301... Val Loss: 1.1300\n",
      "Epoch: 11/20... Step: 3180... Loss: 1.1627... Val Loss: 1.1275\n",
      "Epoch: 11/20... Step: 3190... Loss: 1.1640... Val Loss: 1.1257\n",
      "Epoch: 11/20... Step: 3200... Loss: 1.2173... Val Loss: 1.1281\n",
      "Epoch: 12/20... Step: 3210... Loss: 1.2001... Val Loss: 1.1286\n",
      "Epoch: 12/20... Step: 3220... Loss: 1.2066... Val Loss: 1.1299\n",
      "Epoch: 12/20... Step: 3230... Loss: 1.1914... Val Loss: 1.1315\n",
      "Epoch: 12/20... Step: 3240... Loss: 1.1568... Val Loss: 1.1282\n",
      "Epoch: 12/20... Step: 3250... Loss: 1.2301... Val Loss: 1.1298\n",
      "Epoch: 12/20... Step: 3260... Loss: 1.1742... Val Loss: 1.1297\n",
      "Epoch: 12/20... Step: 3270... Loss: 1.1850... Val Loss: 1.1301\n",
      "Epoch: 12/20... Step: 3280... Loss: 1.1752... Val Loss: 1.1270\n",
      "Epoch: 12/20... Step: 3290... Loss: 1.1577... Val Loss: 1.1289\n",
      "Epoch: 12/20... Step: 3300... Loss: 1.1407... Val Loss: 1.1305\n",
      "Epoch: 12/20... Step: 3310... Loss: 1.1851... Val Loss: 1.1326\n",
      "Epoch: 12/20... Step: 3320... Loss: 1.1718... Val Loss: 1.1313\n",
      "Epoch: 12/20... Step: 3330... Loss: 1.1765... Val Loss: 1.1268\n",
      "Epoch: 12/20... Step: 3340... Loss: 1.1833... Val Loss: 1.1311\n",
      "Epoch: 12/20... Step: 3350... Loss: 1.1912... Val Loss: 1.1295\n",
      "Epoch: 12/20... Step: 3360... Loss: 1.1917... Val Loss: 1.1287\n",
      "Epoch: 12/20... Step: 3370... Loss: 1.2155... Val Loss: 1.1281\n",
      "Epoch: 12/20... Step: 3380... Loss: 1.1933... Val Loss: 1.1269\n",
      "Epoch: 12/20... Step: 3390... Loss: 1.1671... Val Loss: 1.1273\n",
      "Epoch: 12/20... Step: 3400... Loss: 1.1985... Val Loss: 1.1241\n",
      "Epoch: 12/20... Step: 3410... Loss: 1.1646... Val Loss: 1.1272\n",
      "Epoch: 12/20... Step: 3420... Loss: 1.1645... Val Loss: 1.1253\n",
      "Epoch: 12/20... Step: 3430... Loss: 1.1650... Val Loss: 1.1226\n",
      "Epoch: 12/20... Step: 3440... Loss: 1.1520... Val Loss: 1.1228\n",
      "Epoch: 12/20... Step: 3450... Loss: 1.1624... Val Loss: 1.1236\n",
      "Epoch: 12/20... Step: 3460... Loss: 1.1745... Val Loss: 1.1221\n",
      "Epoch: 12/20... Step: 3470... Loss: 1.2020... Val Loss: 1.1199\n",
      "Epoch: 12/20... Step: 3480... Loss: 1.1498... Val Loss: 1.1183\n",
      "Epoch: 12/20... Step: 3490... Loss: 1.1946... Val Loss: 1.1252\n",
      "Epoch: 13/20... Step: 3500... Loss: 1.1910... Val Loss: 1.1266\n",
      "Epoch: 13/20... Step: 3510... Loss: 1.1379... Val Loss: 1.1227\n",
      "Epoch: 13/20... Step: 3520... Loss: 1.1626... Val Loss: 1.1270\n",
      "Epoch: 13/20... Step: 3530... Loss: 1.1457... Val Loss: 1.1208\n",
      "Epoch: 13/20... Step: 3540... Loss: 1.2050... Val Loss: 1.1235\n",
      "Epoch: 13/20... Step: 3550... Loss: 1.1678... Val Loss: 1.1231\n",
      "Epoch: 13/20... Step: 3560... Loss: 1.1845... Val Loss: 1.1234\n",
      "Epoch: 13/20... Step: 3570... Loss: 1.1084... Val Loss: 1.1205\n",
      "Epoch: 13/20... Step: 3580... Loss: 1.1158... Val Loss: 1.1208\n",
      "Epoch: 13/20... Step: 3590... Loss: 1.1448... Val Loss: 1.1215\n",
      "Epoch: 13/20... Step: 3600... Loss: 1.1604... Val Loss: 1.1234\n",
      "Epoch: 13/20... Step: 3610... Loss: 1.1247... Val Loss: 1.1250\n",
      "Epoch: 13/20... Step: 3620... Loss: 1.1147... Val Loss: 1.1236\n",
      "Epoch: 13/20... Step: 3630... Loss: 1.1657... Val Loss: 1.1239\n",
      "Epoch: 13/20... Step: 3640... Loss: 1.1455... Val Loss: 1.1215\n",
      "Epoch: 13/20... Step: 3650... Loss: 1.2004... Val Loss: 1.1206\n",
      "Epoch: 13/20... Step: 3660... Loss: 1.2282... Val Loss: 1.1237\n",
      "Epoch: 13/20... Step: 3670... Loss: 1.1693... Val Loss: 1.1207\n",
      "Epoch: 13/20... Step: 3680... Loss: 1.2054... Val Loss: 1.1176\n",
      "Epoch: 13/20... Step: 3690... Loss: 1.1812... Val Loss: 1.1186\n",
      "Epoch: 13/20... Step: 3700... Loss: 1.1625... Val Loss: 1.1200\n",
      "Epoch: 13/20... Step: 3710... Loss: 1.1499... Val Loss: 1.1164\n",
      "Epoch: 13/20... Step: 3720... Loss: 1.1500... Val Loss: 1.1158\n",
      "Epoch: 13/20... Step: 3730... Loss: 1.1135... Val Loss: 1.1147\n",
      "Epoch: 13/20... Step: 3740... Loss: 1.1971... Val Loss: 1.1141\n",
      "Epoch: 13/20... Step: 3750... Loss: 1.1840... Val Loss: 1.1129\n",
      "Epoch: 13/20... Step: 3760... Loss: 1.1454... Val Loss: 1.1124\n",
      "Epoch: 13/20... Step: 3770... Loss: 1.1557... Val Loss: 1.1103\n",
      "Epoch: 13/20... Step: 3780... Loss: 1.1542... Val Loss: 1.1149\n",
      "Epoch: 14/20... Step: 3790... Loss: 1.1851... Val Loss: 1.1141\n",
      "Epoch: 14/20... Step: 3800... Loss: 1.1822... Val Loss: 1.1146\n",
      "Epoch: 14/20... Step: 3810... Loss: 1.1489... Val Loss: 1.1137\n",
      "Epoch: 14/20... Step: 3820... Loss: 1.1668... Val Loss: 1.1145\n",
      "Epoch: 14/20... Step: 3830... Loss: 1.1994... Val Loss: 1.1170\n",
      "Epoch: 14/20... Step: 3840... Loss: 1.1476... Val Loss: 1.1171\n",
      "Epoch: 14/20... Step: 3850... Loss: 1.1689... Val Loss: 1.1167\n",
      "Epoch: 14/20... Step: 3860... Loss: 1.1613... Val Loss: 1.1169\n",
      "Epoch: 14/20... Step: 3870... Loss: 1.1694... Val Loss: 1.1138\n",
      "Epoch: 14/20... Step: 3880... Loss: 1.1397... Val Loss: 1.1159\n",
      "Epoch: 14/20... Step: 3890... Loss: 1.1596... Val Loss: 1.1160\n",
      "Epoch: 14/20... Step: 3900... Loss: 1.1539... Val Loss: 1.1187\n",
      "Epoch: 14/20... Step: 3910... Loss: 1.1128... Val Loss: 1.1137\n",
      "Epoch: 14/20... Step: 3920... Loss: 1.1529... Val Loss: 1.1147\n",
      "Epoch: 14/20... Step: 3930... Loss: 1.1242... Val Loss: 1.1127\n",
      "Epoch: 14/20... Step: 3940... Loss: 1.1777... Val Loss: 1.1155\n",
      "Epoch: 14/20... Step: 3950... Loss: 1.1470... Val Loss: 1.1180\n",
      "Epoch: 14/20... Step: 3960... Loss: 1.1784... Val Loss: 1.1149\n",
      "Epoch: 14/20... Step: 3970... Loss: 1.1497... Val Loss: 1.1131\n",
      "Epoch: 14/20... Step: 3980... Loss: 1.1465... Val Loss: 1.1136\n",
      "Epoch: 14/20... Step: 3990... Loss: 1.1343... Val Loss: 1.1136\n",
      "Epoch: 14/20... Step: 4000... Loss: 1.1659... Val Loss: 1.1099\n",
      "Epoch: 14/20... Step: 4010... Loss: 1.1393... Val Loss: 1.1121\n",
      "Epoch: 14/20... Step: 4020... Loss: 1.1488... Val Loss: 1.1082\n",
      "Epoch: 14/20... Step: 4030... Loss: 1.1415... Val Loss: 1.1108\n",
      "Epoch: 14/20... Step: 4040... Loss: 1.1874... Val Loss: 1.1100\n",
      "Epoch: 14/20... Step: 4050... Loss: 1.1405... Val Loss: 1.1080\n",
      "Epoch: 14/20... Step: 4060... Loss: 1.1747... Val Loss: 1.1077\n",
      "Epoch: 14/20... Step: 4070... Loss: 1.1742... Val Loss: 1.1121\n",
      "Epoch: 15/20... Step: 4080... Loss: 1.1619... Val Loss: 1.1113\n",
      "Epoch: 15/20... Step: 4090... Loss: 1.1278... Val Loss: 1.1158\n",
      "Epoch: 15/20... Step: 4100... Loss: 1.1281... Val Loss: 1.1115\n",
      "Epoch: 15/20... Step: 4110... Loss: 1.1474... Val Loss: 1.1069\n",
      "Epoch: 15/20... Step: 4120... Loss: 1.1594... Val Loss: 1.1099\n",
      "Epoch: 15/20... Step: 4130... Loss: 1.1218... Val Loss: 1.1107\n",
      "Epoch: 15/20... Step: 4140... Loss: 1.1427... Val Loss: 1.1119\n",
      "Epoch: 15/20... Step: 4150... Loss: 1.1201... Val Loss: 1.1106\n",
      "Epoch: 15/20... Step: 4160... Loss: 1.1524... Val Loss: 1.1098\n",
      "Epoch: 15/20... Step: 4170... Loss: 1.1487... Val Loss: 1.1117\n",
      "Epoch: 15/20... Step: 4180... Loss: 1.1611... Val Loss: 1.1126\n",
      "Epoch: 15/20... Step: 4190... Loss: 1.1275... Val Loss: 1.1107\n",
      "Epoch: 15/20... Step: 4200... Loss: 1.1373... Val Loss: 1.1108\n",
      "Epoch: 15/20... Step: 4210... Loss: 1.1939... Val Loss: 1.1087\n",
      "Epoch: 15/20... Step: 4220... Loss: 1.1272... Val Loss: 1.1112\n",
      "Epoch: 15/20... Step: 4230... Loss: 1.1945... Val Loss: 1.1125\n",
      "Epoch: 15/20... Step: 4240... Loss: 1.1417... Val Loss: 1.1090\n",
      "Epoch: 15/20... Step: 4250... Loss: 1.1516... Val Loss: 1.1087\n",
      "Epoch: 15/20... Step: 4260... Loss: 1.1458... Val Loss: 1.1056\n",
      "Epoch: 15/20... Step: 4270... Loss: 1.1546... Val Loss: 1.1114\n",
      "Epoch: 15/20... Step: 4280... Loss: 1.1313... Val Loss: 1.1091\n",
      "Epoch: 15/20... Step: 4290... Loss: 1.1310... Val Loss: 1.1051\n",
      "Epoch: 15/20... Step: 4300... Loss: 1.1474... Val Loss: 1.1074\n",
      "Epoch: 15/20... Step: 4310... Loss: 1.1934... Val Loss: 1.1061\n",
      "Epoch: 15/20... Step: 4320... Loss: 1.1370... Val Loss: 1.1070\n",
      "Epoch: 15/20... Step: 4330... Loss: 1.1494... Val Loss: 1.1041\n",
      "Epoch: 15/20... Step: 4340... Loss: 1.1253... Val Loss: 1.1035\n",
      "Epoch: 15/20... Step: 4350... Loss: 1.1557... Val Loss: 1.1043\n",
      "Epoch: 15/20... Step: 4360... Loss: 1.1333... Val Loss: 1.1044\n",
      "Epoch: 16/20... Step: 4370... Loss: 1.1243... Val Loss: 1.1092\n",
      "Epoch: 16/20... Step: 4380... Loss: 1.1154... Val Loss: 1.1088\n",
      "Epoch: 16/20... Step: 4390... Loss: 1.1421... Val Loss: 1.1059\n",
      "Epoch: 16/20... Step: 4400... Loss: 1.1168... Val Loss: 1.1040\n",
      "Epoch: 16/20... Step: 4410... Loss: 1.1494... Val Loss: 1.1066\n",
      "Epoch: 16/20... Step: 4420... Loss: 1.1303... Val Loss: 1.1082\n",
      "Epoch: 16/20... Step: 4430... Loss: 1.1798... Val Loss: 1.1057\n",
      "Epoch: 16/20... Step: 4440... Loss: 1.1291... Val Loss: 1.1053\n",
      "Epoch: 16/20... Step: 4450... Loss: 1.1634... Val Loss: 1.1054\n",
      "Epoch: 16/20... Step: 4460... Loss: 1.1245... Val Loss: 1.1072\n",
      "Epoch: 16/20... Step: 4470... Loss: 1.1386... Val Loss: 1.1083\n",
      "Epoch: 16/20... Step: 4480... Loss: 1.1137... Val Loss: 1.1080\n",
      "Epoch: 16/20... Step: 4490... Loss: 1.1341... Val Loss: 1.1061\n",
      "Epoch: 16/20... Step: 4500... Loss: 1.1551... Val Loss: 1.1054\n",
      "Epoch: 16/20... Step: 4510... Loss: 1.1373... Val Loss: 1.1100\n",
      "Epoch: 16/20... Step: 4520... Loss: 1.1462... Val Loss: 1.1080\n",
      "Epoch: 16/20... Step: 4530... Loss: 1.1146... Val Loss: 1.1070\n",
      "Epoch: 16/20... Step: 4540... Loss: 1.1453... Val Loss: 1.1052\n",
      "Epoch: 16/20... Step: 4550... Loss: 1.1316... Val Loss: 1.1057\n",
      "Epoch: 16/20... Step: 4560... Loss: 1.1463... Val Loss: 1.1075\n",
      "Epoch: 16/20... Step: 4570... Loss: 1.1269... Val Loss: 1.1035\n",
      "Epoch: 16/20... Step: 4580... Loss: 1.1203... Val Loss: 1.1018\n",
      "Epoch: 16/20... Step: 4590... Loss: 1.1146... Val Loss: 1.1039\n",
      "Epoch: 16/20... Step: 4600... Loss: 1.1207... Val Loss: 1.1048\n",
      "Epoch: 16/20... Step: 4610... Loss: 1.1241... Val Loss: 1.1035\n",
      "Epoch: 16/20... Step: 4620... Loss: 1.1435... Val Loss: 1.1009\n",
      "Epoch: 16/20... Step: 4630... Loss: 1.1202... Val Loss: 1.0990\n",
      "Epoch: 16/20... Step: 4640... Loss: 1.1207... Val Loss: 1.0994\n",
      "Epoch: 16/20... Step: 4650... Loss: 1.1310... Val Loss: 1.0996\n",
      "Epoch: 17/20... Step: 4660... Loss: 1.1204... Val Loss: 1.0995\n",
      "Epoch: 17/20... Step: 4670... Loss: 1.1254... Val Loss: 1.1051\n",
      "Epoch: 17/20... Step: 4680... Loss: 1.1179... Val Loss: 1.1036\n",
      "Epoch: 17/20... Step: 4690... Loss: 1.1706... Val Loss: 1.1004\n",
      "Epoch: 17/20... Step: 4700... Loss: 1.1258... Val Loss: 1.1016\n",
      "Epoch: 17/20... Step: 4710... Loss: 1.1395... Val Loss: 1.1040\n",
      "Epoch: 17/20... Step: 4720... Loss: 1.1490... Val Loss: 1.1009\n",
      "Epoch: 17/20... Step: 4730... Loss: 1.1089... Val Loss: 1.1049\n",
      "Epoch: 17/20... Step: 4740... Loss: 1.1095... Val Loss: 1.1030\n",
      "Epoch: 17/20... Step: 4750... Loss: 1.0966... Val Loss: 1.1029\n",
      "Epoch: 17/20... Step: 4760... Loss: 1.1386... Val Loss: 1.1061\n",
      "Epoch: 17/20... Step: 4770... Loss: 1.1036... Val Loss: 1.1035\n",
      "Epoch: 17/20... Step: 4780... Loss: 1.1458... Val Loss: 1.1047\n",
      "Epoch: 17/20... Step: 4790... Loss: 1.0920... Val Loss: 1.1012\n",
      "Epoch: 17/20... Step: 4800... Loss: 1.1330... Val Loss: 1.1055\n",
      "Epoch: 17/20... Step: 4810... Loss: 1.1131... Val Loss: 1.1009\n",
      "Epoch: 17/20... Step: 4820... Loss: 1.1025... Val Loss: 1.1026\n",
      "Epoch: 17/20... Step: 4830... Loss: 1.1302... Val Loss: 1.1011\n",
      "Epoch: 17/20... Step: 4840... Loss: 1.1292... Val Loss: 1.1011\n",
      "Epoch: 17/20... Step: 4850... Loss: 1.1122... Val Loss: 1.1020\n",
      "Epoch: 17/20... Step: 4860... Loss: 1.1064... Val Loss: 1.0979\n",
      "Epoch: 17/20... Step: 4870... Loss: 1.1269... Val Loss: 1.0983\n",
      "Epoch: 17/20... Step: 4880... Loss: 1.1210... Val Loss: 1.1025\n",
      "Epoch: 17/20... Step: 4890... Loss: 1.1162... Val Loss: 1.1006\n",
      "Epoch: 17/20... Step: 4900... Loss: 1.0983... Val Loss: 1.0997\n",
      "Epoch: 17/20... Step: 4910... Loss: 1.1403... Val Loss: 1.0971\n",
      "Epoch: 17/20... Step: 4920... Loss: 1.1316... Val Loss: 1.0953\n",
      "Epoch: 17/20... Step: 4930... Loss: 1.1308... Val Loss: 1.0963\n",
      "Epoch: 17/20... Step: 4940... Loss: 1.0792... Val Loss: 1.0955\n",
      "Epoch: 18/20... Step: 4950... Loss: 1.1297... Val Loss: 1.0954\n",
      "Epoch: 18/20... Step: 4960... Loss: 1.1251... Val Loss: 1.0976\n",
      "Epoch: 18/20... Step: 4970... Loss: 1.1450... Val Loss: 1.1034\n",
      "Epoch: 18/20... Step: 4980... Loss: 1.1251... Val Loss: 1.0982\n",
      "Epoch: 18/20... Step: 4990... Loss: 1.1266... Val Loss: 1.0995\n",
      "Epoch: 18/20... Step: 5000... Loss: 1.1022... Val Loss: 1.1011\n",
      "Epoch: 18/20... Step: 5010... Loss: 1.1102... Val Loss: 1.0987\n",
      "Epoch: 18/20... Step: 5020... Loss: 1.0884... Val Loss: 1.0975\n",
      "Epoch: 18/20... Step: 5030... Loss: 1.1050... Val Loss: 1.0960\n",
      "Epoch: 18/20... Step: 5040... Loss: 1.0983... Val Loss: 1.0977\n",
      "Epoch: 18/20... Step: 5050... Loss: 1.1124... Val Loss: 1.1007\n",
      "Epoch: 18/20... Step: 5060... Loss: 1.1014... Val Loss: 1.0996\n",
      "Epoch: 18/20... Step: 5070... Loss: 1.0813... Val Loss: 1.1021\n",
      "Epoch: 18/20... Step: 5080... Loss: 1.0904... Val Loss: 1.1018\n",
      "Epoch: 18/20... Step: 5090... Loss: 1.1367... Val Loss: 1.1018\n",
      "Epoch: 18/20... Step: 5100... Loss: 1.1480... Val Loss: 1.0988\n",
      "Epoch: 18/20... Step: 5110... Loss: 1.0911... Val Loss: 1.1000\n",
      "Epoch: 18/20... Step: 5120... Loss: 1.0952... Val Loss: 1.0999\n",
      "Epoch: 18/20... Step: 5130... Loss: 1.1166... Val Loss: 1.0995\n",
      "Epoch: 18/20... Step: 5140... Loss: 1.1003... Val Loss: 1.0980\n",
      "Epoch: 18/20... Step: 5150... Loss: 1.0769... Val Loss: 1.0963\n",
      "Epoch: 18/20... Step: 5160... Loss: 1.0986... Val Loss: 1.0977\n",
      "Epoch: 18/20... Step: 5170... Loss: 1.1082... Val Loss: 1.0983\n",
      "Epoch: 18/20... Step: 5180... Loss: 1.0874... Val Loss: 1.0937\n",
      "Epoch: 18/20... Step: 5190... Loss: 1.1167... Val Loss: 1.0967\n",
      "Epoch: 18/20... Step: 5200... Loss: 1.1407... Val Loss: 1.0963\n",
      "Epoch: 18/20... Step: 5210... Loss: 1.1239... Val Loss: 1.0939\n",
      "Epoch: 18/20... Step: 5220... Loss: 1.1118... Val Loss: 1.0929\n",
      "Epoch: 18/20... Step: 5230... Loss: 1.1071... Val Loss: 1.0939\n",
      "Epoch: 19/20... Step: 5240... Loss: 1.1433... Val Loss: 1.0933\n",
      "Epoch: 19/20... Step: 5250... Loss: 1.1288... Val Loss: 1.0979\n",
      "Epoch: 19/20... Step: 5260... Loss: 1.1071... Val Loss: 1.0972\n",
      "Epoch: 19/20... Step: 5270... Loss: 1.0802... Val Loss: 1.0977\n",
      "Epoch: 19/20... Step: 5280... Loss: 1.1137... Val Loss: 1.0985\n",
      "Epoch: 19/20... Step: 5290... Loss: 1.1541... Val Loss: 1.0977\n",
      "Epoch: 19/20... Step: 5300... Loss: 1.1273... Val Loss: 1.0965\n",
      "Epoch: 19/20... Step: 5310... Loss: 1.1557... Val Loss: 1.0979\n",
      "Epoch: 19/20... Step: 5320... Loss: 1.1091... Val Loss: 1.0967\n",
      "Epoch: 19/20... Step: 5330... Loss: 1.1128... Val Loss: 1.0951\n",
      "Epoch: 19/20... Step: 5340... Loss: 1.1033... Val Loss: 1.0934\n",
      "Epoch: 19/20... Step: 5350... Loss: 1.0852... Val Loss: 1.0974\n",
      "Epoch: 19/20... Step: 5360... Loss: 1.0762... Val Loss: 1.0984\n",
      "Epoch: 19/20... Step: 5370... Loss: 1.0931... Val Loss: 1.0948\n",
      "Epoch: 19/20... Step: 5380... Loss: 1.0960... Val Loss: 1.1016\n",
      "Epoch: 19/20... Step: 5390... Loss: 1.1171... Val Loss: 1.0967\n",
      "Epoch: 19/20... Step: 5400... Loss: 1.1417... Val Loss: 1.1001\n",
      "Epoch: 19/20... Step: 5410... Loss: 1.1279... Val Loss: 1.1006\n",
      "Epoch: 19/20... Step: 5420... Loss: 1.1202... Val Loss: 1.0964\n",
      "Epoch: 19/20... Step: 5430... Loss: 1.1209... Val Loss: 1.0952\n",
      "Epoch: 19/20... Step: 5440... Loss: 1.1074... Val Loss: 1.0933\n",
      "Epoch: 19/20... Step: 5450... Loss: 1.1502... Val Loss: 1.0978\n",
      "Epoch: 19/20... Step: 5460... Loss: 1.0620... Val Loss: 1.0937\n",
      "Epoch: 19/20... Step: 5470... Loss: 1.0873... Val Loss: 1.0928\n",
      "Epoch: 19/20... Step: 5480... Loss: 1.1143... Val Loss: 1.0937\n",
      "Epoch: 19/20... Step: 5490... Loss: 1.1470... Val Loss: 1.0936\n",
      "Epoch: 19/20... Step: 5500... Loss: 1.1116... Val Loss: 1.0876\n",
      "Epoch: 19/20... Step: 5510... Loss: 1.1229... Val Loss: 1.0892\n",
      "Epoch: 19/20... Step: 5520... Loss: 1.0849... Val Loss: 1.0885\n",
      "Epoch: 20/20... Step: 5530... Loss: 1.1593... Val Loss: 1.0916\n",
      "Epoch: 20/20... Step: 5540... Loss: 1.1024... Val Loss: 1.0926\n",
      "Epoch: 20/20... Step: 5550... Loss: 1.0835... Val Loss: 1.0939\n",
      "Epoch: 20/20... Step: 5560... Loss: 1.1198... Val Loss: 1.0959\n",
      "Epoch: 20/20... Step: 5570... Loss: 1.0992... Val Loss: 1.0937\n",
      "Epoch: 20/20... Step: 5580... Loss: 1.1206... Val Loss: 1.0952\n",
      "Epoch: 20/20... Step: 5590... Loss: 1.0828... Val Loss: 1.0934\n",
      "Epoch: 20/20... Step: 5600... Loss: 1.1054... Val Loss: 1.0959\n",
      "Epoch: 20/20... Step: 5610... Loss: 1.0804... Val Loss: 1.0932\n",
      "Epoch: 20/20... Step: 5620... Loss: 1.0848... Val Loss: 1.0922\n",
      "Epoch: 20/20... Step: 5630... Loss: 1.1331... Val Loss: 1.0954\n",
      "Epoch: 20/20... Step: 5640... Loss: 1.1035... Val Loss: 1.0960\n",
      "Epoch: 20/20... Step: 5650... Loss: 1.0418... Val Loss: 1.0931\n",
      "Epoch: 20/20... Step: 5660... Loss: 1.0662... Val Loss: 1.0942\n",
      "Epoch: 20/20... Step: 5670... Loss: 1.0868... Val Loss: 1.0966\n",
      "Epoch: 20/20... Step: 5680... Loss: 1.1041... Val Loss: 1.0973\n",
      "Epoch: 20/20... Step: 5690... Loss: 1.1345... Val Loss: 1.0947\n",
      "Epoch: 20/20... Step: 5700... Loss: 1.1510... Val Loss: 1.0965\n",
      "Epoch: 20/20... Step: 5710... Loss: 1.1231... Val Loss: 1.0935\n",
      "Epoch: 20/20... Step: 5720... Loss: 1.1095... Val Loss: 1.0934\n",
      "Epoch: 20/20... Step: 5730... Loss: 1.0918... Val Loss: 1.0923\n",
      "Epoch: 20/20... Step: 5740... Loss: 1.0736... Val Loss: 1.0971\n",
      "Epoch: 20/20... Step: 5750... Loss: 1.0876... Val Loss: 1.0909\n",
      "Epoch: 20/20... Step: 5760... Loss: 1.0657... Val Loss: 1.0911\n",
      "Epoch: 20/20... Step: 5770... Loss: 1.0845... Val Loss: 1.0930\n",
      "Epoch: 20/20... Step: 5780... Loss: 1.1087... Val Loss: 1.0920\n",
      "Epoch: 20/20... Step: 5790... Loss: 1.0970... Val Loss: 1.0859\n",
      "Epoch: 20/20... Step: 5800... Loss: 1.1020... Val Loss: 1.0913\n",
      "Epoch: 20/20... Step: 5810... Loss: 1.0899... Val Loss: 1.0899\n",
      "Epoch: 20/20... Step: 5820... Loss: 1.1172... Val Loss: 1.0930\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "seq_length=128\n",
    "n_epochs=20\n",
    "train(network,encoded_char,epochs=n_epochs,batch_size=batch_size,seq_length=seq_length,lr=0.001,print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_model.net'\n",
    "\n",
    "checkpoint = {'n_hidden': network.n_hidden,\n",
    "              'n_layers': network.n_layers,\n",
    "              'state_dict': network.state_dict(),\n",
    "              'tokens': network.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[network.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(network.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = network(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(network.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return network.int2char[char],h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(network, size, prime='Harry', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        network.cuda()\n",
    "    else:\n",
    "        network.cpu()\n",
    "    \n",
    "    network.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = network.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(network, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(network, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-magic people\n",
      "had been allowed to get the strangers of a thing and show we\n",
      "have been trying, he was as is to start this in their words, and he'd be something on his beetle one of them.  He left Harry's friends were supposed to be seen and taken out, at once as someone thought he were still spangling in the corner.\n",
      "\n",
      "\"I were you, Hermione,\" said Harry.\n",
      "\n",
      "Harry saw Harry all the deserted. Hermione was\n",
      "standing on the floor, and he was thinking to the floor to stay\n",
      "when the trapdoor was still sparing in the big.  And then a big land, both tense that the scar had been supporting Hormito Harry that he was.\n",
      "\n",
      "Harry stopped to held him, his hands off as a chocolate shot off\n",
      "the window. The chickens standing their friendly and silence.  He spoke the time to give him a clear, though special weathers, and the common room was the tree, with the staff room.\n",
      "\n",
      "The second. It was the silence. This, Harry couldn't see a laugh, sounded\n",
      "when they were being told. Terrified in the students outside. The wild\n",
      "crack were al\n"
     ]
    }
   ],
   "source": [
    "print(sample(network, 1000, prime='Non-magic people', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
